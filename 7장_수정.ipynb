{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mw6678/git-frist/blob/master/7%EC%9E%A5_%EC%88%98%EC%A0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bQJeXnjSfyg",
        "outputId": "8a749549-fb47-4bf4-a372-d43e95f1a865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word : eat\n",
            "Sense : eat.v.02\n",
            "Definition : eat a meal; take a meal\n",
            "Sentence : They eat a meal\n",
            "Signature Tokens: {'did', 'not', 'until', 'eat', 'there', ',', 'invitation', 'your', 'yet', 'P.M.', 'many', \"n't\", 'gladly', 'We', 'I', 'so', '10', 'because', 'calls', ';', 'take', 'were', 'accept', 'meal', 'phone', 'a'}\n",
            "Overlap with Context: 2\n",
            "Best sense:  Synset('eat.v.02')\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')  # 추가 다운로드\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "\n",
        "def disambiguate(word, sentence, stopwords_set):\n",
        "    # Best sense 를 얻기위한 Lesk 알고리즘을 작성\n",
        "\n",
        "    word_senses = wordnet.synsets(word)\n",
        "    best_sense = word_senses[0]\n",
        "    max_overlap = 0\n",
        "    context = set(word_tokenize(sentence))\n",
        "    for sense in word_senses:\n",
        "        signature = tokenized_gloss(sense)\n",
        "        overlap = compute_overlap(signature, context, stopwords_set)\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = sense\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "def tokenized_gloss(sense):\n",
        "    tokens = set(word_tokenize(sense.definition()))\n",
        "    for example in sense.examples():\n",
        "        tokens = tokens.union(set(word_tokenize(example)))\n",
        "    return tokens\n",
        "\n",
        "def compute_overlap(signature, context, stopwords_set):\n",
        "    gloss = signature.difference(stopwords_set)\n",
        "    return len(gloss.intersection(context))\n",
        "\n",
        "# stopwords 변수 이름을 stopwords_set으로 변경\n",
        "stopwords_set = set(stopwords.words('english'))  # NLTK에서 지정한 영어 불용어 처리 ex) i, my, they...\n",
        "sentence = (\"They eat a meal\")\n",
        "context = set(word_tokenize(sentence))\n",
        "word = 'eat'\n",
        "\n",
        "print(\"Word :\", word)\n",
        "syn = wordnet.synsets('eat')[1]\n",
        "print(\"Sense :\", syn.name())\n",
        "print(\"Definition :\", syn.definition())\n",
        "print(\"Sentence :\", sentence)\n",
        "\n",
        "signature = tokenized_gloss(syn)\n",
        "print(\"Signature Tokens:\", signature)\n",
        "print(\"Overlap with Context:\", compute_overlap(signature, context, stopwords_set))\n",
        "print(\"Best sense: \", disambiguate(word, sentence, stopwords_set))"
      ]
    }
  ]
}